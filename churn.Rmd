---
title: '"Telco Churn Classification"'
author: "Popoola Olawale"
output: html_document
date: "2023-06-27"
---



Predicting Consumer Churn of Telecommunication Firm  Using Classification Techniques
      _Logistic Regression, Decision Tree and Random Forest_

Customer churn happens when customers stop using a product or service from a product. It is a loss for the business. This can lead to loss of profit and market share. This analysis can help a company know more about their customers behaviour and enables companies to implement strategies that increase customer retention rates.

The data to be used is of Telecom industry and gotten from Kaggle. The telecom industry is susceptible to Churn so it is a good example.
This analysis mirrors works that have been done before me. 


 __DATA PREPROCESSING__
 
```{r}
library("dplyr")
library("gridExtra")
library("ggthemes")
library("caret")
library("MASS")
library("randomForest")
library("partykit")


Telco <- read.csv("TelcoCustomerChurn.csv", header = TRUE, stringsAsFactors = TRUE)

```


I attempt to find if there are missing values in the dataset 
```{r}
sapply(Telco, function(x) sum(is.na(x)))
```
I remove the missing values
```{r}
Telco <- Telco[complete.cases(Telco),]
```

# Data wrangling.
Attempting to change some values in col[,c10:15] from 
#1) "No internet service" to "No"
```{r}
col_recode <- c(10:15)
for (i in 1:ncol(Telco[,col_recode])){
   Telco[,col_recode][,i][Telco[,col_recode][,i] == "No internet service"] <- "No"
}
```


#2) Attempting to change the value in Telco$Multiple lines; from "No phone Service" to No
```{r}
Telco$MultipleLines[Telco$MultipleLines == "No phone service"] <- "No"
```

#3) Attempting to change the value in Telco$SeniorCitizens
```{r}
Telco$SeniorCitizen[Telco$SeniorCitizen == 0] <- "No"
Telco$SeniorCitizen[Telco$SeniorCitizen == 1] <- "Yes"

```

#4 I remove the first column of the dataset as the Customer Id is not useful for this classification analysis
```{r}
Telco <- Telco[,-1]
```

__Exploratory Data analysis and Feature selection__

    _Correlation between numeric variables_
```{r}
numeric_Telco <- select_if(Telco, is.numeric)
```

The Tenure Variable is used to quantify the number of months so I remove it.
```{r}
numeric_Telco <- numeric_Telco[,-1]
```

I use correlation to check the similarities between Monthly Charges and Total Charges
```{r}
corr.matrix <- cor(numeric_Telco)
corrplot::corrplot(corr.matrix, main =
  "Correlation Plot for the Numerical Variables")
```

#The Monthly Charges and Total Charges are correlated. So one of them will be removed

```{r}
Telco <- Telco[,-19]
```

__Bar charts Visualisation for categorical variables__
```{r}
p1 <- ggplot(data = Telco)+
  geom_bar(mapping = aes(x = gender, 
            y = 100*(..count..)/sum(..count..)),
           width = 0.5)+
  ggtitle("Gender")+
  ylab("Count")+
  theme_minimal()+
  coord_flip()


p2 <- ggplot(Telco, aes(x=SeniorCitizen)) + ggtitle("Senior Citizen") + xlab("Senior Citizen") + 
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()

p3 <- ggplot(Telco)+
  geom_bar(mapping = aes(x = Partner,
  y = 100*(..count..)/sum(..count..)),
  width = 0.5)+
  ggtitle("Partner")+
  xlab("Partner")+
  ylab("Percentage")+
  theme_minimal()

p4 <- ggplot(Telco)+
  geom_bar(mapping = aes(x = Dependents,
                         y = 100*(..count..)/sum(..count..)),
           width = 0.5)+
  ggtitle("Dependents")+
  xlab("Dependents")+
  ylab("Percentage")+
  theme_minimal()

grid.arrange(p1,p2,p3,p4, ncol =2)
```

The remaining barcharts were constructed for the other variables using the above code.


__Classification algorithms__
  


  _I begin with the logistic regression_

#Data Splitting
```{r}
intrain <- createDataPartition(Telco$Churn, 
            p = 0.7, list = FALSE)
set.seed(2017)

training <- Telco[intrain,]
testing <-  Telco[-intrain,]

dim(training); dim(testing)
```

#Fitting the Logistic Regression Model
```{r}
LogModel <- glm(Churn ~ ., family = binomial
      (link= "logit"), data = training)
summary(LogModel)
```

#Feature Analysis


The top relevant features are Tenure, Contract and PaperBilling.
The next relevant features are PaymentMethodElectronic , SeniorCitizenYes, MultipleLinesYes.
The other relevant features are DependentYes, InternetService,Streaming movies.



#Analysing variance with Anova 
```{r}
anova(LogModel, test = "Chisq")

```

Analysing the deviance table, we can see the drop in deviance when adding each variable one at a time. Tenure and Internet Service significantly reduces the residual variance.



#Assessing the predictive ability of the Logistic Regression model

```{r}
testing$Churn <- as.character(testing$Churn)

testing$Churn[testing$Churn == "No"] <- 0
testing$Churn[testing$Churn == "Yes"] <- 1
testing$Churn <- as.factor(testing$Churn)

fitted.results <- predict(LogModel,newdata = testing, type = 'response')

print("Confusion Matrix for Logistic Regression"); table(testing$Churn, fitted.results > 0.5)
glm_accuracy <- (1397+305)/(1397+151+255+305)
glm_accuracy
```
Accuracy of this model is 0.8074004, 80% 



__Decision Tree__


We are going to use three variables for Decision Trees, they are "Contract", "Tenure', and PaperlessBilling

#Visualisation
```{r}
tree <- ctree(Churn ~ Contract+tenure+PaperlessBilling, training)
plot(tree, type = "simple")
```

#From the plot

1) Contract is the most important feature to churn or not to churn.
2 Users on a year or two contract are less likely to churn whether they are on paper billing.
3) Users with lesss than tenure of 2 months are the most likely to churn.


# Decision Tree Confusion Matrix
```{r}
pred_tree <- predict(tree, testing)
print("confusion Matrix for Decision Tree");table(Predicted = pred_tree, Actual = testing$Churn)
ctree_accuracy = (1355+247)/(1355+313+193+247)
ctree_accuracy
```
The accuracy is 76%





__Random Forest First Model__





```{r}
rfModel <- randomForest(Churn~., training)
rfModel  
```
The error rate is lower when predicting "No" than when predicting "Yes.


```{r}
rf_pred <- predict(rfModel, testing)


print("confusion Matrix for Random Forest");table(Predicted = rf_pred, Actual = testing$Churn)
Rf_accuracy <- (1401+283)/(1401+277+147+283)
print(Rf_accuracy)
```
The model's accuracy is 79%

```{r}

plot(rfModel)
```
The plot showed that the optimal number of trees is from 100 to 200, so I try to tune the model.



# Fit the Random Forest Model after Tuning
```{r}
rfModel_new <- randomForest(Churn ~., data = training, ntree = 200, mtry = 2, importance = TRUE, proximity = TRUE)
print(rfModel_new)


pred_rf_new <- predict(rfModel_new, testing)
table(Predicted = pred_rf_new, Actual = testing$Churn)

pred_rf_new_accuracy <- (1420+268)/(1420+292+128+268)
pred_rf_new_accuracy
```


#Feature Importance according to the Random Forest model
```{r}
varImpPlot(rfModel_new, sort=T, 
    n.var = 10, main = 'Top 10 Feature Importance')
```


From the analysis, I can deduce that the most important features for determining customer churn is Tenure and Contract. The other important features are Paperless billing, Monthly charges, Internet Service. 

Consumers that are on monthly contract with less than 2 month tenure are the most likely to churn. For the company to retain these customers, we need to find the characteristics that makes a customer want to retain the company subscriptions for more than 2 months. Then we can devise the products that encourage these customers to remain with the company.
